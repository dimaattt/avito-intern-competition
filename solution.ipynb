{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5643390a-9565-49b8-9101-8e7a7b0f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers\n",
    "!pip install transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3c52caa-3481-4acb-b01a-0a408751b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transliterate import translit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fa3b0-3187-4ea3-a949-ffc865da1201",
   "metadata": {},
   "source": [
    "# Считывание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b5637-badd-47b1-a284-e954efc92c09",
   "metadata": {},
   "source": [
    "Сначала разберемся с данными, так как разделителем между `id` и `text_no_spaces` является `,`. И она же встречается в качестве знака препинания встречается в `text_no_spaces`, напишем небольшую функцию, которая заменит первую `,` на `;` и считаем датасет указав `sep=';'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baa6973a-4d6f-4750-a571-5462be8dbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_csv(input_file: str)-> pd.DataFrame:\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    with open('clean_dataset.csv', 'w', encoding='utf-8') as outfile:\n",
    "        for line in lines:\n",
    "            modified_line = line.replace(',', ';', 1)\n",
    "            outfile.write(modified_line)\n",
    "\n",
    "    return pd.read_csv('clean_dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "91b299ac-2a90-423e-8310-ece89ec4d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 text_no_spaces\n",
       "0   0                куплюайфон14про\n",
       "1   1             ищудомвПодмосковье\n",
       "2   2  сдаюквартирусмебельюитехникой\n",
       "3   3     новыйдивандоставканедорого\n",
       "4   4                отдамдаромкошку"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = my_read_csv('dataset_1937770_3.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8ea91-36e2-45c6-b867-89abf9e5e61b",
   "metadata": {},
   "source": [
    "# Создание частотного словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77959dd3-fadb-48fc-b95b-1a80a1513369",
   "metadata": {},
   "source": [
    "Можно было догадаться (посмотреть внимательно на датасет), что в текстах часто встречаются английские слова, причем почти всегда это какие-то бренды. Поэтому логично будет написать функцию, которая принимает на вход наши текста и возвращает набор только брендов + названия этих брендов на кирилице (на всякий случай)\n",
    "\n",
    "\n",
    "\n",
    "P.S.: Дефолтный перевод в кириллицу работает __фигово__, починим, если останется время.. Вообще кажется, что самый популярный англицизм в данной задаче это __айфон__. Поэтому его добавим руками в словарь (Надеюсь это не бан :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "350d4416-5130-4d10-9cdb-c79d89a617cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_transliterate_english_words(data):\n",
    "    eng_words = set()\n",
    "    translit_eng_words = set()\n",
    "    for line in data: \n",
    "        # Ищем английские слова \n",
    "        english_words = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?![a-z])', line)\n",
    "           \n",
    "        for word in english_words:\n",
    "            lower_word = word.lower()\n",
    "            # Заменяем на кирилицу\n",
    "            translit_eng = translit(lower_word, 'ru')\n",
    "            \n",
    "            eng_words.add(lower_word)\n",
    "            translit_eng_words.add(translit_eng)\n",
    "    \n",
    "    return eng_words | translit_eng_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "215ead6f-72c5-473d-a171-695c60ff6d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adidas',\n",
       " 'micri',\n",
       " 'one',\n",
       " 'philip',\n",
       " 'slim',\n",
       " 'адидас',\n",
       " 'мицри',\n",
       " 'оне',\n",
       " 'пхилип',\n",
       " 'слим'}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_and_transliterate_english_words(['куплютелевизорPhilipOneMICRI4slimкуплюкроссовкиAdidas,оригинал'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcbbdf-e709-4025-823f-4d8adf8b3762",
   "metadata": {},
   "source": [
    "Вроде даже работает\n",
    "\n",
    "Итак это была подготовка к созданию словаря, в него я загружу словарь русских слов взятый из [гитхаба](https://github.com/danakt/russian-words)\n",
    "\n",
    "Также добавим в этот словарь все бренды на английском и их кирилицу + АЙФОН"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "df5a6c32-51db-48c3-a8f4-1482c245395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_russian_frequency_dict(data):\n",
    "    url = 'https://raw.githubusercontent.com/danakt/russian-words/master/russian.txt'\n",
    "    response = requests.get(url)\n",
    "    words = response.text.splitlines()\n",
    "    \n",
    "    brand_words = list(find_and_transliterate_english_words(data))\n",
    "    brand_words.append('айфон')\n",
    "    \n",
    "    numbers = [str(i) for i in range(0, 10000)]\n",
    "    \n",
    "    words = set(words + brand_words + numbers)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5eed5-7aee-45ea-bc45-1aa62925fdfd",
   "metadata": {},
   "source": [
    "# Подготовка токинезатора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f774b-b7a1-4f98-a225-fe4e1b674729",
   "metadata": {},
   "source": [
    "Тут даже не знаю, что можно подробно рассказать. \n",
    "\n",
    "Создаем токенизатор и загружаем в него корпус из часто употребляемых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "bd24cc22-1193-4b2d-b995-7fd0d1e684ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bpe_tokenizer():\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    \n",
    "    # Используем корпус русских слов для обучения\n",
    "    corpus = [\"куплю\", \"ищу\", \"cниму\", \"продам\", \"бу\", \"торг\", 'айфон']\n",
    "    \n",
    "    tokenizer.train_from_iterator(corpus, trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243108f-f36b-4a0b-8bc5-0971c8031557",
   "metadata": {},
   "source": [
    "Напищем еще функцию, которая разбивает строку на куски, причем в роли сепаратора  цифры и знаки препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "51faf567-3560-4761-9b1b-074e4b495245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    pattern = r'''\n",
    "        [A-Z][a-z]+(?:[A-Z][a-z]+)*|  # Слова с CamelCase: HelloWorld, iPhone\n",
    "        [А-ЯЁ][а-яё]+(?:[А-ЯЁ][а-яё]+)*|  # Русские слова с заглавными: ПриветМир\n",
    "        [a-z]+|  # английские слова в нижнем регистре\n",
    "        [а-яё]+|  # русские слова в нижнем регистре\n",
    "        [A-Z]+|  # XYZ\n",
    "        [А-ЯЁ]+|  # РУССКИЕ АББРЕВИАТУРЫ\n",
    "        \\d+|  # числа\n",
    "        [^a-zA-Zа-яА-ЯёЁ\\d\\s]  # знаки препинания\n",
    "    '''\n",
    "    \n",
    "    tokens = re.findall(pattern, text, re.VERBOSE)\n",
    "    \n",
    "    # Дополнительная обработка для случаев, когда слова слиплись без изменения регистра\n",
    "    refined_tokens = []\n",
    "    for token in tokens:\n",
    "        if (re.match(r'^[a-zA-Zа-яА-ЯёЁ]+$', token) and \n",
    "            not re.match(r'^([A-ZА-ЯЁ][a-zа-яё]+)+$', token) and\n",
    "            not token.isupper() and not token.islower()):\n",
    "            \n",
    "            subtokens = re.split(r'(?<=[a-zа-яё])(?=[A-ZА-ЯЁ])|(?<=[A-ZА-ЯЁ])(?=[a-zа-яё])', token)\n",
    "            refined_tokens.extend(subtokens)\n",
    "        else:\n",
    "            refined_tokens.append(token)\n",
    "    \n",
    "    return refined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "c12c53ae-47da-4184-acac-79c61ed7f157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сейчас',\n",
       " '2025',\n",
       " 'год',\n",
       " 'Приветземля',\n",
       " ',',\n",
       " 'алоало',\n",
       " 'adsfsdf',\n",
       " ',',\n",
       " 'dfgdsg',\n",
       " 'Samsund',\n",
       " 'DF',\n",
       " 'sd']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text('Сейчас2025годПриветземля,алоалоadsfsdf,dfgdsgSamsundDFsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "c321dedf-f5cb-421d-81af-b4a1598b0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_preprocess_text(text):\n",
    "    original_text = text\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "    processed_text = ' '.join(tokens)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    return processed_text, original_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6e9b0-586e-4c6a-a2c1-4b002de5763c",
   "metadata": {},
   "source": [
    "Данная функция для восстановления текста в исходный вид, потому что в решении я буду все приводить в нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "da913022-a086-41b7-b5f8-1ebf99d9d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_case(text, original_text):\n",
    "    result = []\n",
    "    words = text.split()\n",
    "    original_lower = original_text.lower()\n",
    "    \n",
    "    for word in words:\n",
    "        # Ищем позицию слова в оригинальном тексте\n",
    "        start_pos = original_lower.find(word.lower())\n",
    "        if start_pos != -1:\n",
    "            original_word = original_text[start_pos:start_pos+len(word)]\n",
    "            if original_word.isupper():\n",
    "                result.append(word.upper())\n",
    "            elif original_word[0].isupper() and original_word[1:].islower():\n",
    "                result.append(word.capitalize())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    \n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3d96e-e3e9-4d01-bb85-d53c9c78ff31",
   "metadata": {},
   "source": [
    "# Основная часть. Динамическое программирование "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4bc4e-1f9f-4b84-a9fd-801ae050da44",
   "metadata": {},
   "source": [
    "Что тут происходит по шагам:\n",
    "1. Инициализация DP\n",
    "2. Рассматриваем все префиксы строки длины `i` (от 1 до n)\n",
    "3. Перебираем все возможные места начала последнего слова в префиксе. Для каждой пары `(j, i)` формируем подстроку `word = text_lower[j:i]`\n",
    "4. Если подстрока целиком есть в словаре нашем построеном словаре `russian_and_brand_dict`, то считаем её \"хорошим\" словом\n",
    "5. Если подстрока не найдена в словарe, то пытаемся разобрать подстроку с помощью BPE-токенизатора.\n",
    "   1. Если токенизатор пометил слово как неизвестно, даём сильный штраф\n",
    "   2. Если BPE смог разбить подстроку на знакомые подслова, даём небольшое положительное вознаграждение\n",
    "6. Сохраняем новую лучшую оценку и запомним точку разрыва `j`\n",
    "7. Обработка односимвольного хвоста (Важно). Потому что как оказалось случай __5B__ может разбить на мелкие слова и оставить один символ в конце предложения, т.е. очевидно неправильное разбиение\n",
    "8. Вызываем внешнюю функцию `process_case`, которая восстанавливает регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "39aa5fa1-8786-4f79-a4df-1821303699f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_spaces(text: str) -> str:\n",
    "    original_text = text\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    n = len(text_lower)\n",
    "\n",
    "    # Инициализация массива динамического программирования\n",
    "    dp = [-np.inf] * (n + 1)\n",
    "    dp[0] = 0\n",
    "    prev = [0] * (n + 1)\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i):\n",
    "            word = text_lower[j:i]\n",
    "            \n",
    "            if word in russian_and_brand_dict:\n",
    "                score = dp[j] + len(word)**2 \n",
    "            else:\n",
    "                # Проверяем через BPE токенизатор\n",
    "                tokens = bpe_tokenizer.encode(word).tokens\n",
    "                if len(tokens) == 1 and tokens[0] == '[UNK]':\n",
    "                    score = -10  \n",
    "                else:\n",
    "                    score = dp[j] + 1 \n",
    "            \n",
    "            if score > dp[i]:\n",
    "                dp[i] = score\n",
    "                prev[i] = j\n",
    "    \n",
    "    # Восстанавливаем разбиение\n",
    "    words = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        j = prev[i]\n",
    "        words.append(text_lower[j:i])\n",
    "        i = j\n",
    "    \n",
    "    words.reverse()\n",
    "    \n",
    "    # Обрабатываем случай с одной буквой в конце\n",
    "    if len(words) > 1 and len(words[-1]) == 1:\n",
    "        words[-2] += words[-1]\n",
    "        words.pop()\n",
    "    \n",
    "    result = ' '.join(words)\n",
    "    result = process_case(result, original_text)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f4cb0307-82e1-4552-86b0-b95fd0dc2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset: List[str]) -> List[str]:\n",
    "    return [restore_spaces(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8eabf-8515-4503-a1f0-3cca944c3dbd",
   "metadata": {},
   "source": [
    "# Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03703a-9770-4273-9892-433c98c1c09c",
   "metadata": {},
   "source": [
    "Тут приведены некоторые ~костыли~ эвристики, если кратко, то чтобы в конце предложения не оставалась одна буква, чтобы обрабатывались окончания, всякие дефисы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "55e7cd88-685f-4850-82dd-3ebc6caa5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_advanced(text):\n",
    "    patterns = {\n",
    "        1: (re.compile(r',\\s*([а-яё])\\s+'), r', \\1'),\n",
    "        2: (re.compile(r'(\\w+)\\s+(\\w)([^\\w\\s]*)$'), r'\\1\\2\\3'),\n",
    "        3: (re.compile(r'(\\d)\\s+(\\d)'), r'\\1\\2'),\n",
    "        4: (re.compile(r'(\\w+)\\s+([а-яё])\\s*,'), r'\\1\\2,'),\n",
    "        5: (re.compile(r'(\\w+)\\s+(ая|ый|ий|яя|ое|ее|ой|ей|ую|юю|ом|ем|им|ым|ах|ях|ами|ями|ов|ев|ей|ий)\\b'), r'\\1\\2'),\n",
    "        6: (re.compile(r'-\\s+'), '-'),\n",
    "        7: (re.compile(r\"\\s*['']\\s*\"), \"'\")\n",
    "    }\n",
    "    \n",
    "    patterns_order = [1, 2, 3, 4, 5, 6, 7]\n",
    "    \n",
    "    cleaned = text\n",
    "    \n",
    "    for pattern_num in patterns_order:\n",
    "        if pattern_num in patterns:\n",
    "            pattern, replacement = patterns[pattern_num]\n",
    "            cleaned = pattern.sub(replacement, cleaned)\n",
    "    \n",
    "    # Дополнительная очистка пробелов\n",
    "    cleaned = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "b8c2f52a-6ca5-44ce-90ea-ec52a11da987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 40.8 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = df['text_no_spaces']\n",
    "\n",
    "russian_and_brand_dict = load_russian_frequency_dict(text)\n",
    "bpe_tokenizer = load_bpe_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02dd46-b3e0-4290-938a-0b970a89c13a",
   "metadata": {},
   "source": [
    "Напишем простенькую функцию проставки идексов пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "f1813d51-6366-4510-9ec6-9e3dff6d58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_positions(original_text, restored_text):\n",
    "    restored_no_spaces = restored_text.replace(' ', '')\n",
    "    \n",
    "    if restored_no_spaces != original_text:\n",
    "        # Если тексты не совпадают, пытаемся найти наилучшее соответствие\n",
    "        # Это может произойти, если есть ошибки в восстановлении\n",
    "        min_len = min(len(restored_no_spaces), len(original_text))\n",
    "        restored_no_spaces = restored_no_spaces[:min_len]\n",
    "        original_text = original_text[:min_len]\n",
    "    \n",
    "    space_positions = []\n",
    "    # i - индекс в restored_text, j - индекс в original_text\n",
    "    i, j = 0, 0  \n",
    "    \n",
    "    while i < len(restored_text) and j < len(original_text):\n",
    "        if restored_text[i] == ' ':\n",
    "            space_positions.append(j)\n",
    "            i += 1\n",
    "        elif restored_text[i] == original_text[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return space_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "4fb08298-0d50-4c65-a1c3-62f51ef8dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.95 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans_arr = []\n",
    "restored = process_dataset(text)\n",
    "\n",
    "for original, pred_result in zip(text, restored):\n",
    "    result = clean_text_advanced(pred_result)\n",
    "    ans_arr.append(get_space_positions(original, result))\n",
    "    # print(f'Original: {original}\\nRestored: {result} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991abcfe-98cc-4d59-b72f-a499dbb617ad",
   "metadata": {},
   "source": [
    "## Что по времени"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eac208-6b01-47c1-ae70-b4ca8d93c35f",
   "metadata": {},
   "source": [
    "Получилось достаточно быстро, `bpe_tokenizer` + `inference` = 49.7 s + 5.19 s = 54.89 s (результаты могут отличаться от запуска к запуску, но не сильно)\n",
    "\n",
    "Вообще специально избегал каких-то больших и страшных моделей, потому что очевидно данная задача должна быстро отрабатывать на инференсе, обучаться желательно тоже, именно поэтому не был загружен словарь английских слов, он тут просто не нужен, достаточно было бренды детектить по \"нерусским\" буквам + они часто начинаются с большой буквы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "5aefc473-7c80-4568-aa80-d0f0d79e411e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "      <th>predicted_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "      <td>[5, 10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "      <td>[3, 6, 7, 10, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "      <td>[4, 12, 13, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "      <td>[5, 10, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "      <td>[5, 10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 text_no_spaces  predicted_positions\n",
       "0   0                куплюайфон14про          [5, 10, 12]\n",
       "1   1             ищудомвПодмосковье    [3, 6, 7, 10, 16]\n",
       "2   2  сдаюквартирусмебельюитехникой  [4, 12, 13, 20, 21]\n",
       "3   3     новыйдивандоставканедорого          [5, 10, 18]\n",
       "4   4                отдамдаромкошку              [5, 10]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['predicted_positions'] = ans_arr\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "d2644ad7-e13c-4949-bcdb-aa69dffb616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ans.txt', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4b9c8-e9b0-48fd-926e-4e3ef76a8c88",
   "metadata": {},
   "source": [
    "Получили `Your Mean F1 = 90.27%`. \n",
    "\n",
    "Как можно улучшить:\n",
    "1. Сделать качественную предобработку, надо было в саааамом начале это делать, я уже не успеваю, думаю процента 3-5 там закопано\n",
    "2. Может пересмотреть модель и выбрать модель попроще\n",
    "3. Сделать какую-то вторичное расставление пробелов/удаление пробелов, не просто как я по регуляркам, а что-то УмнОе\n",
    "\n",
    "---\n",
    "__Спасибо организаторам__ за такую возможность проявить себя и за такой датасет, под эти песни и пытался разгадать загадку пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b5e37-7a50-473b-98b6-9806f13a0049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
